{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "model_name = \"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\" \n",
    "max_seq_length = 8192 # Qwen handles context very well\n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "# 1. Load the Model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# 2. Add LoRA Adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\", \n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "# 3. Load Data & Format\n",
    "dataset = load_dataset(\"json\", data_files=\"./llama4_podcast_arts_1k_v2.jsonl\", split=\"train\")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "# 4. Train\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, \n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,\n",
    "        max_steps = 375, # 375 steps is usually enough for style transfer\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"llama3_podcast_outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# 5. Save\n",
    "model.save_pretrained(\"llama3_4b_podcast_lora\")\n",
    "tokenizer.save_pretrained(\"llama3_4b_podcast_lora\")\n",
    "\n",
    "print(\"ðŸ’¾ Saving q4_k_m (Fast / Mobile Version)...\")\n",
    "model.save_pretrained_gguf(\n",
    "    \"podcast_llama3_q4\", \n",
    "    tokenizer, \n",
    "    quantization_method = \"q4_k_m\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "model_name = \"unsloth/gemma-3-4b-it\" \n",
    "max_seq_length = 8192 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "# 1. Load the Model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# 2. Add LoRA Adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\", \n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "# 3. Load Data & Format\n",
    "dataset = load_dataset(\"json\", data_files=\"./llama4_podcast_arts_1k_v2.jsonl\", split=\"train\")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = []\n",
    "    \n",
    "    for convo in convos:\n",
    "        # 1. Standardize roles for Gemma 3 (User -> Assistant -> User...)\n",
    "        gemma_convo = []\n",
    "        for i, msg in enumerate(convo):\n",
    "            # We map Alex/Jamie to User/Assistant based on turn index\n",
    "            # Turn 0, 2, 4... become 'user'\n",
    "            # Turn 1, 3, 5... become 'model' (Gemma's assistant role)\n",
    "            role = \"user\" if i % 2 == 0 else \"model\"\n",
    "            \n",
    "            # Prepend the character name so the model learns WHO is talking\n",
    "            content = f\"{msg['role'].upper()}: {msg['content']}\"\n",
    "            gemma_convo.append({\"role\": role, \"content\": content})\n",
    "            \n",
    "        text = tokenizer.apply_chat_template(gemma_convo, tokenize=False, add_generation_prompt=False)\n",
    "        texts.append(text)\n",
    "        \n",
    "    return { \"text\" : texts }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "# 4. Train\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, \n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,\n",
    "        max_steps = 375,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"gemma3_podcast_outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# 5. Save\n",
    "model.save_pretrained(\"gemma3_4b_podcast_lora\")\n",
    "tokenizer.save_pretrained(\"gemma3_4b_podcast_lora\")\n",
    "\n",
    "print(\"ðŸ’¾ Saving q4_k_m (Fast / Mobile Version)...\")\n",
    "model.save_pretrained_gguf(\n",
    "    \"podcast_gemma3_q4\", \n",
    "    tokenizer, \n",
    "    quantization_method = \"q4_k_m\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "# 1. Memory Management Environment Variable\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "model_name = \"unsloth/gemma-3-4b-it\"\n",
    "max_seq_length = 8192\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# 1. Load the Model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# 2. Add LoRA Adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\", # Optimized for VRAM\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "# 3. Load Data & Format\n",
    "dataset = load_dataset(\"json\", data_files=\"./llama4_podcast_arts_1k_v2.jsonl\", split=\"train\")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = []\n",
    "    for convo in convos:\n",
    "        gemma_convo = []\n",
    "        for i, msg in enumerate(convo):\n",
    "            role = \"user\" if i % 2 == 0 else \"model\"\n",
    "            content = f\"{msg['role'].upper()}: {msg['content']}\"\n",
    "            gemma_convo.append({\"role\": role, \"content\": content})\n",
    "\n",
    "        text = tokenizer.apply_chat_template(gemma_convo, tokenize=False, add_generation_prompt=False)\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "\n",
    "# 4. Training Arguments - CRITICAL CHANGES\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Clear cache before starting\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1, # Reduced from 2 to 1\n",
    "        gradient_accumulation_steps = 8, # Doubled to keep effective batch size at 8\n",
    "        warmup_steps = 10,\n",
    "        max_steps = 375,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"gemma3_podcast_outputs\",\n",
    "        save_total_limit = 2, # Don't fill up disk with checkpoints\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# 5. Save Model\n",
    "model.save_pretrained(\"gemma3_4b_podcast_lora\")\n",
    "tokenizer.save_pretrained(\"gemma3_4b_podcast_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilling Dual Map: Neuralink\n",
      "Distilling Dual Map: Quantum Cryptography\n",
      "Distilling Dual Map: Autonomous Vehicles\n",
      "Distilling Dual Map: Transhumanism\n",
      "Distilling Dual Map: Starlink\n",
      "Distilling Dual Map: Space Mining\n",
      "Distilling Dual Map: Nuclear Fusion\n",
      "Distilling Dual Map: Placebo Effect\n",
      "Distilling Dual Map: Circadian Rhythm\n",
      "Distilling Dual Map: Growth Mindset\n",
      "Distilling Dual Map: Lucid Dreaming\n",
      "Distilling Dual Map: Attachment Theory\n",
      "Distilling Dual Map: Biohacking\n",
      "Distilling Dual Map: Cold Hydrotherapy\n",
      "Distilling Dual Map: Flow State\n",
      "Distilling Dual Map: Microbiome\n",
      "Distilling Dual Map: Inflation\n",
      "Distilling Dual Map: Hyperinflation\n",
      "Distilling Dual Map: Compound Interest\n",
      "Distilling Dual Map: Stock Market Crash 1929\n",
      "Distilling Dual Map: Bitcoin\n",
      "Distilling Dual Map: GameStop Squeeze\n",
      "Distilling Dual Map: Universal Basic Income\n",
      "Distilling Dual Map: Fiat Money\n",
      "Distilling Dual Map: Venture Capital\n"
     ]
    }
   ],
   "source": [
    "import json, random, time, re, wikipediaapi\n",
    "from groq import Groq\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "GROQ_API_KEY = \"\"\n",
    "MASTER_TEACHER = \"openai/gpt-oss-120b\" \n",
    "\n",
    "DOMAINS = {\n",
    "    # \"True Crime\": [\"Zodiac Killer\", \"D. B. Cooper\", \"JonBenÃ©t Ramsey\", \"Lizzie Borden\", \"Black Dahlia\", \"Jack the Ripper\", \"Sherlock Holmes\", \"Golden State Killer\", \"Gardner Museum Heist\"],\n",
    "    \"Future Tech\": [\n",
    "        # \"Metaverse\", \"Web3\",\n",
    "         \"Neuralink\", \"Quantum Cryptography\", \"Autonomous Vehicles\", \"Transhumanism\", \"Starlink\", \"Space Mining\", \"Nuclear Fusion\"],\n",
    "    \"Health\": [\"Placebo Effect\", \"Circadian Rhythm\", \"Growth Mindset\", \"Lucid Dreaming\", \"Attachment Theory\", \"Biohacking\", \"Cold Hydrotherapy\", \"Flow State\", \"Microbiome\"],\n",
    "    \"Finance\": [\"Inflation\", \"Hyperinflation\", \"Compound Interest\", \"Stock Market Crash 1929\", \"Bitcoin\", \"GameStop Squeeze\", \"Universal Basic Income\", \"Fiat Money\", \"Venture Capital\"]\n",
    "}\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "wiki = wikipediaapi.Wikipedia(user_agent='JamiePod/v23.0', language='en')\n",
    "\n",
    "# ================= REFINERY UTILS =================\n",
    "\n",
    "def split_content(text):\n",
    "    \"\"\"Separates <think> tags from dialogue and scrubs prefixes/labels.\"\"\"\n",
    "    think_match = re.search(r'<think>(.*?)</think>', text, re.DOTALL)\n",
    "    thought = f\"<think>{think_match.group(1).strip()}</think>\" if think_match else \"\"\n",
    "    \n",
    "    dial = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()\n",
    "    # Remove Speaker Prefixes (e.g., 'ALEX:', 'JAMIE:')\n",
    "    dial = re.sub(r'^(ALEX:|JAMIE:|Turn \\d+:)', '', dial, flags=re.IGNORECASE).strip()\n",
    "    return thought, dial\n",
    "\n",
    "def generate_master_script(topic, context):\n",
    "    \"\"\"Teacher generates the 'Golden Copy' using a more robust prompt structure.\"\"\"\n",
    "    model = MASTER_TEACHER\n",
    "    \n",
    "    # We use a structured prompt to avoid token collision\n",
    "    prompt = f\"\"\"\n",
    "    ### SYSTEM INSTRUCTIONS\n",
    "    You are a world-class podcast scriptwriter. \n",
    "    TASK: Write a 10-turn dialogue between ALEX (Host) and JAMIE (Expert) about {topic}.\n",
    "    \n",
    "    ### TOPIC CONTEXT\n",
    "    {context[:2500]}\n",
    "    \n",
    "    ### FORMATTING RULES\n",
    "    1. Every single turn MUST follow this exact pattern:\n",
    "       SPEAKER: <think>Plan the response and check facts here</think> The actual dialogue goes here.\n",
    "    2. JAMIE: Uses analogies, stays grounded.\n",
    "    3. ALEX: Skeptical, asks 'How' and 'Why'.\n",
    "    4. NO numbered lists. NO placeholders.\n",
    "    \n",
    "    ### OUTPUT BEGINS NOW\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        res = client.chat.completions.create(\n",
    "            messages=[{\"role\":\"user\",\"content\":prompt}], \n",
    "            model=model, \n",
    "            temperature=0.75,\n",
    "            # We add a max_tokens limit to ensure the model doesn't cut off early\n",
    "            max_tokens=4000 \n",
    "        )\n",
    "        content = res.choices[0].message.content\n",
    "        if not content:\n",
    "            print(f\"Warning: Model returned empty content for {topic}\")\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"Groq API Error on {topic}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ================= THE DUAL-MAPPER =================\n",
    "\n",
    "def save_dual_datasets(turns, topic):\n",
    "    \"\"\"Creates two separate JSONL files by 'flipping' roles.\"\"\"\n",
    "    \n",
    "    # Target Roles\n",
    "    configurations = [\n",
    "        {\"file\": \"jamie_qwen.jsonl\", \"assistant_name\": \"JAMIE\", \"user_name\": \"ALEX\"},\n",
    "        {\"file\": \"alex_qwen.jsonl\", \"assistant_name\": \"ALEX\", \"user_name\": \"JAMIE\"}\n",
    "    ]\n",
    "\n",
    "    for config in configurations:\n",
    "        fname = config[\"file\"]\n",
    "        target = config[\"assistant_name\"]\n",
    "        opponent = config[\"user_name\"]\n",
    "\n",
    "        with open(fname, \"a\") as f:\n",
    "            for i in range(len(turns)):\n",
    "                speaker, thought, dialogue = turns[i]\n",
    "                \n",
    "                # We only save a sample when the CURRENT speaker is the one we are training\n",
    "                if speaker == target:\n",
    "                    messages = [{\"role\": \"system\", \"content\": f\"You are {target} discussing {topic}.\"}]\n",
    "                    \n",
    "                    # History Window (Last 4 turns)\n",
    "                    for j in range(max(0, i - 4), i):\n",
    "                        h_spk, _, h_dl = turns[j]\n",
    "                        # If history turn was the target, it's 'assistant'. If opponent, it's 'user'.\n",
    "                        role = \"assistant\" if h_spk == target else \"user\"\n",
    "                        messages.append({\"role\": role, \"content\": h_dl})\n",
    "                    \n",
    "                    # The Target Turn: Dialogue + Thought (The Reasoning Trace)\n",
    "                    final_content = f\"{thought}\\n\\n{dialogue}\"\n",
    "                    messages.append({\"role\": \"assistant\", \"content\": final_content})\n",
    "                    \n",
    "                    f.write(json.dumps({\"messages\": messages}) + \"\\n\")\n",
    "\n",
    "# ================= EXECUTION =================\n",
    "\n",
    "for domain, topics in DOMAINS.items():\n",
    "    for topic in topics:\n",
    "        print(f\"Distilling Dual Map: {topic}\")\n",
    "        page = wiki.page(topic)\n",
    "        script = generate_master_script(topic, page.summary[:2500] if page.exists() else \"\")\n",
    "        \n",
    "        if script:\n",
    "            turns = []\n",
    "            for line in script.split('\\n'):\n",
    "                if \":\" not in line: continue\n",
    "                spk, raw = line.split(\":\", 1)\n",
    "                th, dl = split_content(raw)\n",
    "                if dl: turns.append((spk.strip().upper(), th, dl))\n",
    "            \n",
    "            # This generates BOTH Jamie and Alex files in one pass\n",
    "            save_dual_datasets(turns, topic)\n",
    "        \n",
    "        time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install Unsloth & Dependencies\n",
    "# !pip install unsloth vllm bitsandbytes\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# 2. Load the Qwen-4B Model (Optimized for Thinking/Reasoning)\n",
    "max_seq_length = 4096 # Supports up to 128k, but 4k is perfect for 10-turn scripts\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B-Instruct-2507-unsloth-bnb-4bit\", # Latest Qwen3 4B build\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# 3. Add LoRA Adapters (Rank 32 for jamie's Persona Depth)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Lowered from 64 to prevent memorization of 220 rows\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32, # Matched to r\n",
    "    lora_dropout = 0.05, # Added a tiny bit of dropout to help generalization\n",
    "    bias = \"none\",    \n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "# 4. Configure the ChatML Template (Crucial for <think> tags)\n",
    "# This template tells the model where jamie ends and jamie begins\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\", # Native support for reasoning tags\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"messages\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in instructions]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "# 5. Load your JSONL Data\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"/kaggle/input/podcast-conversation-data/jamie_reason.jsonl\", split=\"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "# 6. Set up the Trainer with Response-Only Masking\n",
    "# This is the \"Magic\" line that masks out jamie's prompts\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,      # Lowered for VRAM\n",
    "        gradient_accumulation_steps = 8,     # Increased to compensate\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 100, \n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",              \n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "        # Fixes the 'int' mean error and saves VRAM\n",
    "        average_tokens_across_devices = False,\n",
    "        # Extreme VRAM saving (may slow down training slightly)\n",
    "        gradient_checkpointing = True, \n",
    "    ),\n",
    ")\n",
    "\n",
    "# Apply the mask: model is only graded on what comes after <|im_start|>assistant\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|im_start|>user\\n\",\n",
    "    response_part = \"<|im_start|>assistant\\n\",\n",
    ")\n",
    "\n",
    "# 7. Train!\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"qwen3_4b_podcast_jamie_lora\")\n",
    "tokenizer.save_pretrained(\"qwen3_4b_podcast_jamie_lora\")\n",
    "\n",
    "# Save directly to GGUF (Optimized for Ollama/LM Studio)\n",
    "print(\"ðŸ’¾ Saving q4_k_m (Fast / Mobile Version)...\")\n",
    "model.save_pretrained_gguf(\n",
    "    \"podcast_qwen3_jamie_q4km\", \n",
    "    tokenizer, \n",
    "    quantization_method = \"q4_k_m\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
